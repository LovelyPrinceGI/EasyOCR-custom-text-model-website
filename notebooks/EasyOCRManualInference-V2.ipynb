{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "623aac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./EasyOCR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c430a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Error: Character set file not found at './models/synth/custom_char.txt'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/synth/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# model_path = f'./models/synth/checkpoint_10.pth'\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Initialize converter with your custom character set\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m converter \u001b[38;5;241m=\u001b[39m \u001b[43mCTCLabelConverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcharacter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m num_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(converter\u001b[38;5;241m.\u001b[39mcharacter)\n\u001b[1;32m     35\u001b[0m ignore_idx \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/easyocr/utils.py:278\u001b[0m, in \u001b[0;36mCTCLabelConverter.__init__\u001b[0;34m(self, character, separator_list, dict_pathlist)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, character, separator_list \u001b[38;5;241m=\u001b[39m {}, dict_pathlist \u001b[38;5;241m=\u001b[39m {}):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# character (str): set of the possible characters.\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m     dict_character \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcharacter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dict_character):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from easyocr.utils import CTCLabelConverter\n",
    "from easyocr.config import recognition_models\n",
    "\n",
    "# === Step 1: Load Your Custom Character Set from File ===\n",
    "def load_character_set(file_path='./models/synth/custom_char.txt'):\n",
    "    \"\"\"Loads the character set from a text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            # .strip() removes any potential leading/trailing whitespace\n",
    "            characters = f.read()\n",
    "        return characters\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸ Error: Character set file not found at '{file_path}'\")\n",
    "        return None\n",
    "    \n",
    "character = load_character_set()\n",
    "\n",
    "# === Step 2: Define Model Path and Parameters ===\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_config = recognition_models['gen1']['thai_g1']\n",
    "model_path = f'./models/synth/{model_config[\"filename\"]}'\n",
    "# model_path = f'./models/synth/checkpoint_10.pth'\n",
    "\n",
    "# Initialize converter with your custom character set\n",
    "converter = CTCLabelConverter(character)\n",
    "num_class = len(converter.character)\n",
    "ignore_idx = []\n",
    "\n",
    "# Network parameters must match your trained model's architecture\n",
    "network_params = {\n",
    "    'input_channel': 1,\n",
    "    'output_channel': 512,\n",
    "    'hidden_size': 512,\n",
    "    'num_class': num_class  # Pass the correct number of classes\n",
    "}\n",
    "\n",
    "\n",
    "# === Step 3: Build the Model and Load Your Weights ===\n",
    "model_pkg = importlib.import_module(\"easyocr.model.model\")\n",
    "model = model_pkg.Model(**network_params)\n",
    "\n",
    "# Load the saved weights\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Remove 'module.' prefix if it exists (from DataParallel training)\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] if k.startswith('module.') else k\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "# Load the weights into the model\n",
    "# Use strict=True because the model architecture and state_dict should match perfectly.\n",
    "model.load_state_dict(new_state_dict, strict=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Custom model loaded successfully for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fcf78c9-fca5-413c-a14c-b3754243f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 4: Preprocessing ===\n",
    "def preprocess_image(pil_img, contrast_factor=1.0):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 600)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    return transform(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "def custom_mean(probs):\n",
    "    return float(torch.mean(probs).item()) if len(probs) > 0 else 0.0\n",
    "\n",
    "# === Step 5: Prediction Function (EasyOCR-style) ===\n",
    "def predict_text(image_path, contrast_ths=0.1, adjust_contrast=0.5, decoder='greedy', beamWidth=5):\n",
    "    pil_img = Image.open(image_path).convert('L')\n",
    "    return _predict_with_contrast_retry(pil_img, contrast_ths, adjust_contrast, decoder, beamWidth)\n",
    "\n",
    "def _predict_with_contrast_retry(pil_img, contrast_ths, adjust_contrast, decoder, beamWidth):\n",
    "    decoded_text, confidence = _predict(pil_img, decoder, beamWidth, contrast_factor=1.0)\n",
    "    if confidence < contrast_ths:\n",
    "        decoded_text, confidence = _predict(pil_img, decoder, beamWidth, contrast_factor=adjust_contrast)\n",
    "    return decoded_text, confidence\n",
    "\n",
    "def _predict(pil_img, decoder='greedy', beamWidth=5, contrast_factor=1.0):\n",
    "    image_tensor = preprocess_image(pil_img, contrast_factor)\n",
    "    batch_size = image_tensor.size(0)\n",
    "    batch_max_length = 25\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_for_pred = torch.LongTensor(batch_size, batch_max_length + 1).fill_(0).to(device)\n",
    "        preds = model(image_tensor, text_for_pred)  # [B, C, T]\n",
    "        preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "\n",
    "        probs = F.softmax(preds, dim=2)\n",
    "        probs[:, :, ignore_idx] = 0.0  # filter out ignored characters\n",
    "        norm = probs.sum(dim=2, keepdim=True)\n",
    "        probs = probs / norm\n",
    "\n",
    "        if decoder == 'greedy':\n",
    "            _, preds_index = probs.max(2)\n",
    "            preds_index_flat = preds_index.view(-1).cpu().numpy()\n",
    "            preds_str = converter.decode_greedy(preds_index_flat, preds_size.cpu())[0]\n",
    "        elif decoder == 'beamsearch':\n",
    "            preds_np = probs.cpu().numpy()\n",
    "            preds_str = converter.decode_beamsearch(preds_np, beamWidth=beamWidth)[0]\n",
    "        elif decoder == 'wordbeamsearch':\n",
    "            preds_np = probs.cpu().numpy()\n",
    "            preds_str = converter.decode_wordbeamsearch(preds_np, beamWidth=beamWidth)[0]\n",
    "\n",
    "        # Confidence calculation\n",
    "        values, indices = probs.max(2)\n",
    "        mask = indices != 0  # ignore blank tokens\n",
    "        filtered = [v[m] for v, m in zip(values, mask)]\n",
    "        confidence = custom_mean(filtered[0]) if filtered else 0.0\n",
    "\n",
    "    return preds_str, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b81921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Predicted Text: à¸™à¸«à¸ªà¹Œà¸—à¹à¸›à¸°à¸¢à¹ˆà¹‚à¸›à¸£à¸‡à¸§à¸´à¸™à¸à¸¸à¸ˆà¸¨à¸ˆà¸«à¸™à¸™à¸˜à¸´à¸—à¸—à¸²à¹†à¸«à¸¥à¸·à¸­à¸Šà¸¹à¹‰\n",
      "ðŸ“ˆ Confidence: 0.74\n"
     ]
    }
   ],
   "source": [
    "# === Step 6: Run Prediction ===\n",
    "image_path = './Receipts/test_tiny.jpg'\n",
    "text, conf = predict_text(image_path)\n",
    "print(f\"ðŸ“ Predicted Text: {text}\")\n",
    "print(f\"ðŸ“ˆ Confidence: {conf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4b155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Predicted Text: à¸šà¸£à¸´à¸©à¸±à¸—à¹‚à¸Ÿà¸¥à¸§à¹Œà¹à¸­à¸„à¹€à¸„à¸²à¸—à¹Œà¸—à¸”à¸ªà¸­à¸š\n",
      "ðŸ“ˆ Confidence: 0.96\n"
     ]
    }
   ],
   "source": [
    "image_path = './Receipts/clear_sample.jpg'\n",
    "text, conf = predict_text(image_path)\n",
    "print(f\"ðŸ“ Predicted Text: {text}\")\n",
    "print(f\"ðŸ“ˆ Confidence: {conf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6807f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Clean Thai text: à¸šà¸£à¸´à¸©à¸±à¸—à¹‚à¸Ÿà¸¥à¸§à¹Œà¹à¸­à¸„à¹€à¸„à¸²à¸—à¹Œà¸—à¸”à¸ªà¸­à¸š\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def keep_thai_eng_num(text):\n",
    "    # Thai: \\u0E00-\\u0E7F\n",
    "    # English letters: a-zA-Z\n",
    "    # Digits: 0-9\n",
    "    return ''.join(re.findall(r'[\\u0E00-\\u0E7Fa-zA-Z0-9]+', text))\n",
    "\n",
    "\n",
    "raw_text = text\n",
    "clean_text = keep_thai_eng_num(raw_text)\n",
    "print(f\"ðŸ“ Clean Thai text: {clean_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
